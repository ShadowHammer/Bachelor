{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2241,
     "status": "ok",
     "timestamp": 1710153242265,
     "user": {
      "displayName": "Aksel Lytzen",
      "userId": "15669626429637119241"
     },
     "user_tz": -60
    },
    "id": "K0Q43AE7A7Qp",
    "outputId": "a87c49b4-ddbc-45f3-eb96-17fc6f0168c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n",
      "0.17.1+cu121\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pycocotools'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 32\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(torchvision\u001b[38;5;241m.\u001b[39m__version__)\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpycocotools\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcoco\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m COCO\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01malbumentations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ToTensorV2\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#from google.colab import drive\u001b[39;00m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m#drive.mount('/content/drive')\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pycocotools'"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A  # our data augmentation library\n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # progress bar\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%matplotlib inline\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "from pycocotools.coco import COCO\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# pip install pycocotools numpy pandas matplotlib albumentations torch torchvision pytorch-lightning opencv-python matplotlib\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparamaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nt9MdSbf9Cjf"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs= 1\n",
    "lr = 0.001\n",
    "image_size = [600, 600]\n",
    "is_Test = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cvY5964A7Qt"
   },
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(image_size[0], image_size[1]), # our input size can be 600px\n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(image_size[0], image_size[1]), # our input size can be 600px\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GYvDf_eSA7Qu"
   },
   "outputs": [],
   "source": [
    "class PotholeDetectionClass(datasets.VisionDataset):\n",
    "    def __init__(self, root, stage='/train', transform=None, target_transform=None, transforms=None, batch_size = batch_size):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.stage = stage #train, valid, test\n",
    "        self.coco = COCO(root + stage + \"/_annotations.coco.json\") # annotations stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        path = \"/\" + path\n",
    "        image = cv2.imread(self.root + self.stage + path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "\n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target]\n",
    "        if self.transforms is not None:\n",
    "            transformed = self.transforms(image=image, bboxes=boxes)\n",
    "\n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "\n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "\n",
    "        targ = {}\n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n",
    "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        return image.div(255), targ # scale images\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        train_dataset = self.root + \"/train\"\n",
    "        return DataLoader(train_dataset, batch_size=self.batch_size, num_workers=0, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        valid_dataset = self.root + \"/valid\"\n",
    "        return DataLoader(valid_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False)\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        test_dataset = self.root + \"/test\"\n",
    "        return DataLoader(test_dataset, batch_size=self.batch_size, num_workers=0, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ClH7TyviA7Qv"
   },
   "outputs": [],
   "source": [
    "#dataset_path = \"/content/drive/MyDrive/Uni/Github/Bachelor/Pytorch_lightning/Pothole_coco\"\n",
    "dataset_path = \"/Pothole_coco\"\n",
    "dataset_path = os.getcwd() + dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 28,
     "status": "ok",
     "timestamp": 1710153242268,
     "user": {
      "displayName": "Aksel Lytzen",
      "userId": "15669626429637119241"
     },
     "user_tz": -60
    },
    "id": "35C0EOuUA7Qw",
    "outputId": "ad652db8-565a-4c2c-daec-093f018c8191"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{0: {'id': 0, 'name': 'potholes', 'supercategory': 'none'},\n",
       " 1: {'id': 1, 'name': 'pothole', 'supercategory': 'potholes'}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coco = COCO(dataset_path + \"/train\" + \"/_annotations.coco.json\")\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUU5_xkoA7Qx"
   },
   "outputs": [],
   "source": [
    "classes = [i[1]['name'] for i in categories.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1710153242269,
     "user": {
      "displayName": "Aksel Lytzen",
      "userId": "15669626429637119241"
     },
     "user_tz": -60
    },
    "id": "uCKhDX95A7Q3",
    "outputId": "3514aea9-684b-4a0c-81d5-9cdf65effd03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "train_dataset = PotholeDetectionClass(root=dataset_path, transforms=get_transforms(True))\n",
    "test_dataset = PotholeDetectionClass(root=dataset_path, stage='/test', transforms=get_transforms(False))\n",
    "valid_dataset = PotholeDetectionClass(root=dataset_path, stage= \"/valid\", transform=get_transforms(False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PASOujyb9Cj0"
   },
   "source": [
    "# Plot image with predetermined bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zv3HNbFAA7Q4"
   },
   "outputs": [],
   "source": [
    "#sample = train_dataset[6]\n",
    "#img_int = torch.tensor(sample[0] * 255, dtype=torch.uint8)\n",
    "#plt.imshow(draw_bounding_boxes(\n",
    "#    img_int, sample[1]['boxes'], [classes[i] for i in sample[1]['labels']], width=4\n",
    "#).permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzeKaiFnA7Q5"
   },
   "outputs": [],
   "source": [
    "model = models.detection.fasterrcnn_mobilenet_v3_large_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TF2wrvYvA7Q6"
   },
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cSQ1HTZUA7Q7"
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=0, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(valid_dataset, batch_size=16, shuffle=False, num_workers=0, collate_fn=custom_collate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlkDCqQ69CkG"
   },
   "source": [
    "Run to test if training works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RJ4ckeoLA7Q8"
   },
   "outputs": [],
   "source": [
    "#images,targets = next(iter(train_loader))\n",
    "#images = list(image for image in images)\n",
    "#targets = [{k:v for k, v in t.items()} for t in targets]\n",
    "#output = model(images, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Uk67EGi9CkI"
   },
   "source": [
    "Set device to Cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SOcN0qNUlXL"
   },
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\") # use GPU to train\n",
    "device = \"cpu\"\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nb4sPyN8UlXM"
   },
   "outputs": [],
   "source": [
    "# Now, and optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, train_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.losses_dict = []\n",
    "        self.loss = nn.HingeEmbeddingLoss()\n",
    "\n",
    "    def forward(self, images, targets):\n",
    "        return self.model(images, targets)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        images = torch.stack([image.to(self.device) for image in images])\n",
    "        targets = [{k: torch.tensor(v).to(self.device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = self.model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "        self.losses_dict.append({k: v.item() for k, v in loss_dict.items()})\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        images = list(image.to(self.device) for image in images)\n",
    "        targets = [{k: torch.tensor(v).to(self.device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = self.model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        loss_value = losses.item()\n",
    "\n",
    "        self.losses_dict.append({k: v.item() for k, v in loss_dict.items()})\n",
    "\n",
    "        if not math.isfinite(loss_value):\n",
    "            print(f\"Loss is {loss_value}, stopping training\")\n",
    "            print(loss_dict)\n",
    "            sys.exit(1)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "    def load_from_checkpoint(cls, checkpoint_path, model, optimizer, train_loader, test_loader):\n",
    "        model = model.load_from_checkpoint(checkpoint_path)\n",
    "        return cls(model, optimizer, train_loader, test_loader)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "error",
     "timestamp": 1710153243724,
     "user": {
      "displayName": "Aksel Lytzen",
      "userId": "15669626429637119241"
     },
     "user_tz": -60
    },
    "id": "JKIwGld1UlXP",
    "outputId": "97ea2c03-f77f-4571-d581-145385b55684"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type               | Params\n",
      "---------------------------------------------\n",
      "0 | model | FasterRCNN         | 18.9 M\n",
      "1 | loss  | HingeEmbeddingLoss | 0     \n",
      "---------------------------------------------\n",
      "18.9 M    Trainable params\n",
      "58.9 K    Non-trainable params\n",
      "18.9 M    Total params\n",
      "75.721    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/30 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tobia\\AppData\\Local\\Temp\\ipykernel_19588\\2160212632.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targets = [{k: torch.tensor(v).to(self.device) for k, v in t.items()} for t in targets]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [00:54<00:00,  0.55it/s, v_num=10]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [00:54<00:00,  0.55it/s, v_num=10]\n"
     ]
    }
   ],
   "source": [
    "#for epoch in range(num_epochs):\n",
    "#    train_one_epoch(model, optimizer, train_loader, device, epoch)\n",
    "\n",
    "\n",
    "lightning_module = NN(model, optimizer, train_loader, test_loader)\n",
    "# Initialize a Lightning Trainer\n",
    "trainer = pl.Trainer(max_epochs=num_epochs)  # You can adjust the Trainer options\n",
    "\n",
    "# Start training\n",
    "trainer.fit(lightning_module, train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQCucnv_9CkP"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "[{'loss_classifier': 0.6224026679992676, 'loss_box_reg': 0.2880636155605316, 'loss_objectness': 0.32039645314216614, 'loss_rpn_box_reg': 0.02279277890920639}, {'loss_classifier': 0.24826236069202423, 'loss_box_reg': 0.28460079431533813, 'loss_objectness': 0.1149812638759613, 'loss_rpn_box_reg': 0.014959920197725296}, {'loss_classifier': 0.28416433930397034, 'loss_box_reg': 0.31022411584854126, 'loss_objectness': 0.1291336566209793, 'loss_rpn_box_reg': 0.02203456684947014}, {'loss_classifier': 0.27589404582977295, 'loss_box_reg': 0.3082258701324463, 'loss_objectness': 0.11295930296182632, 'loss_rpn_box_reg': 0.013785094022750854}, {'loss_classifier': 0.2545846998691559, 'loss_box_reg': 0.30065301060676575, 'loss_objectness': 0.11376892030239105, 'loss_rpn_box_reg': 0.02285582758486271}, {'loss_classifier': 0.19994382560253143, 'loss_box_reg': 0.23665636777877808, 'loss_objectness': 0.11744051426649094, 'loss_rpn_box_reg': 0.01841791719198227}, {'loss_classifier': 0.22127345204353333, 'loss_box_reg': 0.32302385568618774, 'loss_objectness': 0.0766322985291481, 'loss_rpn_box_reg': 0.016098130494356155}, {'loss_classifier': 0.22517867386341095, 'loss_box_reg': 0.3275865316390991, 'loss_objectness': 0.06989830732345581, 'loss_rpn_box_reg': 0.018030237406492233}, {'loss_classifier': 0.22234472632408142, 'loss_box_reg': 0.3177383840084076, 'loss_objectness': 0.11998042464256287, 'loss_rpn_box_reg': 0.021010752767324448}, {'loss_classifier': 0.21611559391021729, 'loss_box_reg': 0.32965582609176636, 'loss_objectness': 0.0639616921544075, 'loss_rpn_box_reg': 0.01393727958202362}, {'loss_classifier': 0.16919724643230438, 'loss_box_reg': 0.2195661962032318, 'loss_objectness': 0.07527051866054535, 'loss_rpn_box_reg': 0.0108574777841568}, {'loss_classifier': 0.2044619619846344, 'loss_box_reg': 0.2889330983161926, 'loss_objectness': 0.06610989570617676, 'loss_rpn_box_reg': 0.013773507438600063}, {'loss_classifier': 0.14366771280765533, 'loss_box_reg': 0.2354416400194168, 'loss_objectness': 0.0465841144323349, 'loss_rpn_box_reg': 0.009234853088855743}, {'loss_classifier': 0.1879439353942871, 'loss_box_reg': 0.2664838433265686, 'loss_objectness': 0.1057116836309433, 'loss_rpn_box_reg': 0.01828538253903389}, {'loss_classifier': 0.1592114120721817, 'loss_box_reg': 0.25314417481422424, 'loss_objectness': 0.04754931479692459, 'loss_rpn_box_reg': 0.011316662654280663}, {'loss_classifier': 0.19542741775512695, 'loss_box_reg': 0.25886794924736023, 'loss_objectness': 0.06094052642583847, 'loss_rpn_box_reg': 0.01385076530277729}, {'loss_classifier': 0.175206258893013, 'loss_box_reg': 0.22088094055652618, 'loss_objectness': 0.06678009033203125, 'loss_rpn_box_reg': 0.014771380461752415}, {'loss_classifier': 0.22099460661411285, 'loss_box_reg': 0.3024255633354187, 'loss_objectness': 0.04713375121355057, 'loss_rpn_box_reg': 0.012573016807436943}, {'loss_classifier': 0.18224914371967316, 'loss_box_reg': 0.23251737654209137, 'loss_objectness': 0.04898163676261902, 'loss_rpn_box_reg': 0.013178201392292976}, {'loss_classifier': 0.16158220171928406, 'loss_box_reg': 0.21812093257904053, 'loss_objectness': 0.041558362543582916, 'loss_rpn_box_reg': 0.012943826615810394}, {'loss_classifier': 0.2241334319114685, 'loss_box_reg': 0.2871350049972534, 'loss_objectness': 0.06904691457748413, 'loss_rpn_box_reg': 0.02117118425667286}, {'loss_classifier': 0.19244197010993958, 'loss_box_reg': 0.223800390958786, 'loss_objectness': 0.05659330636262894, 'loss_rpn_box_reg': 0.01273744273930788}, {'loss_classifier': 0.14572378993034363, 'loss_box_reg': 0.22529324889183044, 'loss_objectness': 0.044761773198843, 'loss_rpn_box_reg': 0.01296314224600792}, {'loss_classifier': 0.14638304710388184, 'loss_box_reg': 0.19559018313884735, 'loss_objectness': 0.05016307160258293, 'loss_rpn_box_reg': 0.015371142886579037}, {'loss_classifier': 0.1739473044872284, 'loss_box_reg': 0.2606103718280792, 'loss_objectness': 0.028357818722724915, 'loss_rpn_box_reg': 0.007979992777109146}, {'loss_classifier': 0.15719012916088104, 'loss_box_reg': 0.19887876510620117, 'loss_objectness': 0.035696178674697876, 'loss_rpn_box_reg': 0.012749891728162766}, {'loss_classifier': 0.1804465502500534, 'loss_box_reg': 0.22337692975997925, 'loss_objectness': 0.0441233292222023, 'loss_rpn_box_reg': 0.012914642691612244}, {'loss_classifier': 0.17542794346809387, 'loss_box_reg': 0.2583151161670685, 'loss_objectness': 0.043200671672821045, 'loss_rpn_box_reg': 0.025157421827316284}, {'loss_classifier': 0.13548396527767181, 'loss_box_reg': 0.16457876563072205, 'loss_objectness': 0.045144811272621155, 'loss_rpn_box_reg': 0.006668922025710344}, {'loss_classifier': 0.16568711400032043, 'loss_box_reg': 0.24738889932632446, 'loss_objectness': 0.004449876490980387, 'loss_rpn_box_reg': 0.0033544048201292753}]\n",
      "-----------------------------\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "NN.load_from_checkpoint() missing 1 required positional argument: 'test_loader'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 20\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-----------------------------\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#trainer = pl.Trainer(max_epochs=num_epochs+2)\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m#model.eval()\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#trainer = trainer.fit(lightning_module,ckpt_path=ckpt_path)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#lightning_module = NN(model, optimizer, train_loader, test_loader, is_Test=True)\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#load_checkpoint = NN.load_from_checkpoint(ckpt_path, model = model, optimizer = optimizer, train_loader = train_loader, test_loader = test_loader)\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m load_checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m trainer \u001b[38;5;241m=\u001b[39m pl\u001b[38;5;241m.\u001b[39mTrainer()\n\u001b[0;32m     22\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtest(load_checkpoint)\n",
      "\u001b[1;31mTypeError\u001b[0m: NN.load_from_checkpoint() missing 1 required positional argument: 'test_loader'"
     ]
    }
   ],
   "source": [
    "ckpt_path = \"lightning_logs/version_0/checkpoints/epoch=0-step=30.ckpt\"\n",
    "#model = pl.LightningModule.load_from_checkpoint(ckpt_path)\n",
    "#print losses_dict\n",
    "print (\"-----------------------------\")\n",
    "print(lightning_module.losses_dict)\n",
    "print (\"-----------------------------\")\n",
    "#trainer = pl.Trainer(max_epochs=num_epochs+2)\n",
    "#model.eval()\n",
    "#trainer = trainer.fit(lightning_module,ckpt_path=ckpt_path)\n",
    "\n",
    "#model = Net()\n",
    "#optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "#checkpoint = torch.load(ckpt_path)\n",
    "#model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#epoch = checkpoint['epoch']\n",
    "#loss = checkpoint['loss']\n",
    "#lightning_module = NN(model, optimizer, train_loader, test_loader, is_Test=True)\n",
    "#load_checkpoint = NN.load_from_checkpoint(ckpt_path, model = model, optimizer = optimizer, train_loader = train_loader, test_loader = test_loader)\n",
    "load_checkpoint = NN.load_from_checkpoint(ckpt_path, model, optimizer, train_loader, test_loader)\n",
    "trainer = pl.Trainer()\n",
    "trainer.test(load_checkpoint)\n",
    "#is_Test = True\n",
    "#trainer.test(ckpt_path=ckpt_path,dataloaders=train_loader)\n",
    "\n",
    "\n",
    "##    img-245_jpg.rf.1c9b49a366bda1cf64dbfac2a946cd38.jpg\n",
    "\n",
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x_max is less than or equal to x_min for bbox (0.68, 0.5433333333333333, 0.68, 0.5466666666666666, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mPotholeDetectionClass.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     25\u001b[0m boxes \u001b[38;5;241m=\u001b[39m [t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m image \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     30\u001b[0m boxes \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\composition.py:219\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[0;32m    214\u001b[0m check_each_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(item\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_each_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    216\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 219\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[0;32m    222\u001b[0m     data \u001b[38;5;241m=\u001b[39m t(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\utils.py:82\u001b[0m, in \u001b[0;36mDataProcessor.preprocess\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     80\u001b[0m rows, cols \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_fields:\n\u001b[1;32m---> 82\u001b[0m     data[data_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_and_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\utils.py:92\u001b[0m, in \u001b[0;36mDataProcessor.check_and_convert\u001b[1;34m(self, data, rows, cols, direction)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_from_albumentations(data, rows, cols)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py:140\u001b[0m, in \u001b[0;36mBboxProcessor.convert_to_albumentations\u001b[1;34m(self, data, rows, cols)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_albumentations\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Sequence[BoxType], rows: \u001b[38;5;28mint\u001b[39m, cols: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BoxType]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_bboxes_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_validity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py:410\u001b[0m, in \u001b[0;36mconvert_bboxes_to_albumentations\u001b[1;34m(bboxes, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_bboxes_to_albumentations\u001b[39m(\n\u001b[0;32m    407\u001b[0m     bboxes: Sequence[BoxType], source_format: \u001b[38;5;28mstr\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m, cols: \u001b[38;5;28mint\u001b[39m, check_validity: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BoxType]:\n\u001b[0;32m    409\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mconvert_bbox_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_validity\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes]\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py:354\u001b[0m, in \u001b[0;36mconvert_bbox_to_albumentations\u001b[1;34m(bbox, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[0;32m    352\u001b[0m     bbox \u001b[38;5;241m=\u001b[39m normalize_bbox(bbox, rows, cols)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_validity:\n\u001b[1;32m--> 354\u001b[0m     \u001b[43mcheck_bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bbox\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py:440\u001b[0m, in \u001b[0;36mcheck_bbox\u001b[1;34m(bbox)\u001b[0m\n\u001b[0;32m    438\u001b[0m x_min, y_min, x_max, y_max \u001b[38;5;241m=\u001b[39m bbox[:\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_max \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m x_min:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_max is less than or equal to x_min for bbox \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbox\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_max \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y_min:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_max is less than or equal to y_min for bbox \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbox\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x_max is less than or equal to x_min for bbox (0.68, 0.5433333333333333, 0.68, 0.5466666666666666, 1)."
     ]
    }
   ],
   "source": [
    "test_dataset[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf-FZUnM9CkQ"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x_max is less than or equal to x_min for bbox (0.68, 0.5433333333333333, 0.68, 0.5466666666666666, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m9\u001b[39m,\u001b[38;5;241m10\u001b[39m): \u001b[38;5;66;03m#test_dataset.__len__()-30\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "Cell \u001b[1;32mIn[4], line 27\u001b[0m, in \u001b[0;36mPotholeDetectionClass.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m     25\u001b[0m boxes \u001b[38;5;241m=\u001b[39m [t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbbox\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m [t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory_id\u001b[39m\u001b[38;5;124m'\u001b[39m]] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m target]\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     transformed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mboxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m image \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     30\u001b[0m boxes \u001b[38;5;241m=\u001b[39m transformed[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\composition.py:219\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, force_apply, *args, **data)\u001b[0m\n\u001b[0;32m    214\u001b[0m check_each_transform \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28mgetattr\u001b[39m(item\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_each_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mvalues()\n\u001b[0;32m    216\u001b[0m )\n\u001b[0;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocessors\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m--> 219\u001b[0m     \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m transforms:\n\u001b[0;32m    222\u001b[0m     data \u001b[38;5;241m=\u001b[39m t(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdata)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\utils.py:82\u001b[0m, in \u001b[0;36mDataProcessor.preprocess\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     80\u001b[0m rows, cols \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_fields:\n\u001b[1;32m---> 82\u001b[0m     data[data_name] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_and_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdata_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdirection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\utils.py:92\u001b[0m, in \u001b[0;36mDataProcessor.check_and_convert\u001b[1;34m(self, data, rows, cols, direction)\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m data\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m direction \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_from_albumentations(data, rows, cols)\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py:140\u001b[0m, in \u001b[0;36mBboxProcessor.convert_to_albumentations\u001b[1;34m(self, data, rows, cols)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_albumentations\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Sequence[BoxType], rows: \u001b[38;5;28mint\u001b[39m, cols: \u001b[38;5;28mint\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BoxType]:\n\u001b[1;32m--> 140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_bboxes_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_validity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py:410\u001b[0m, in \u001b[0;36mconvert_bboxes_to_albumentations\u001b[1;34m(bboxes, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_bboxes_to_albumentations\u001b[39m(\n\u001b[0;32m    407\u001b[0m     bboxes: Sequence[BoxType], source_format: \u001b[38;5;28mstr\u001b[39m, rows: \u001b[38;5;28mint\u001b[39m, cols: \u001b[38;5;28mint\u001b[39m, check_validity: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    408\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[BoxType]:\n\u001b[0;32m    409\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a list bounding boxes from a format specified in `source_format` to the format used by albumentations\"\"\"\u001b[39;00m\n\u001b[1;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mconvert_bbox_to_albumentations\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_format\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_validity\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m bbox \u001b[38;5;129;01min\u001b[39;00m bboxes]\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py:354\u001b[0m, in \u001b[0;36mconvert_bbox_to_albumentations\u001b[1;34m(bbox, source_format, rows, cols, check_validity)\u001b[0m\n\u001b[0;32m    352\u001b[0m     bbox \u001b[38;5;241m=\u001b[39m normalize_bbox(bbox, rows, cols)\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_validity:\n\u001b[1;32m--> 354\u001b[0m     \u001b[43mcheck_bbox\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bbox\n",
      "File \u001b[1;32mc:\\Users\\tobia\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\albumentations\\core\\bbox_utils.py:440\u001b[0m, in \u001b[0;36mcheck_bbox\u001b[1;34m(bbox)\u001b[0m\n\u001b[0;32m    438\u001b[0m x_min, y_min, x_max, y_max \u001b[38;5;241m=\u001b[39m bbox[:\u001b[38;5;241m4\u001b[39m]\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_max \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m x_min:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx_max is less than or equal to x_min for bbox \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbox\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_max \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m y_min:\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_max is less than or equal to y_min for bbox \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbbox\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: x_max is less than or equal to x_min for bbox (0.68, 0.5433333333333333, 0.68, 0.5466666666666666, 1)."
     ]
    }
   ],
   "source": [
    "\n",
    "print(test_dataset[9])\n",
    "for i in range(9,10): #test_dataset.__len__()-30\n",
    "    print(i)\n",
    "    print(test_dataset[i])\n",
    "    img, _ = test_dataset[i]\n",
    "    img_int = torch.tensor(img*255, dtype=torch.uint8)\n",
    "    with torch.no_grad():\n",
    "        prediction = model([img.to(device)])\n",
    "        pred = prediction[0]\n",
    "        fig = plt.figure(figsize=(14, 10))\n",
    "    plt.imshow(draw_bounding_boxes(img_int,\n",
    "        pred['boxes'][pred['scores'] > 0.8],\n",
    "        [classes[i] for i in pred['labels'][pred['scores'] > 0.8].tolist()], width=4\n",
    "    ).permute(1, 2, 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DI1qRAJC9CkR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
