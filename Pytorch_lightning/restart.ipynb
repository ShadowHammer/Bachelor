{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1+cu121\n",
      "0.17.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "from torchvision.transforms import functional as FT\n",
    "from torchvision import transforms as T\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import torchmetrics\n",
    "from torchmetrics import Metric\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, sampler, random_split, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torchvision.datasets import CocoDetection\n",
    "import copy\n",
    "import math\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import albumentations as A \n",
    "from collections import defaultdict, deque\n",
    "import datetime\n",
    "import time\n",
    "from tqdm import tqdm # progress bar\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "%matplotlib inline\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)\n",
    "from pycocotools.coco import COCO\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "#import wandb\n",
    "import matplotlib.patches as patches\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper paramaters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 664,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "num_epochs= 1\n",
    "lr = 0.001\n",
    "image_size = [600, 600]\n",
    "is_Test = False\n",
    "wandb_on = False\n",
    "device_cuda = False\n",
    "num_workers = 0\n",
    "if wandb_on:\n",
    "    wandb.login()\n",
    "\n",
    "    wandb.init(\n",
    "        # set the wandb project where this run will be logged\n",
    "        project=\"Bachelor0386\",\n",
    "        \n",
    "        # track hyperparameters and run metadata\n",
    "        config={\n",
    "        \"architecture\": \"Faster RCNN\",\n",
    "        \"dataset\": \"CustomDataset\",\n",
    "        \"epochs\": 1,\n",
    "        }\n",
    "    )\n",
    "\n",
    "if device_cuda == True:\n",
    "    print(\"Using GPU\")\n",
    "    device = torch.device(\"cuda\") # use GPU to train\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "    device = \"cpu\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_transforms(train=False):\n",
    "    if train:\n",
    "        transform = A.Compose([\n",
    "            A.Resize(image_size[0], image_size[1]), \n",
    "            A.HorizontalFlip(p=0.3),\n",
    "            A.VerticalFlip(p=0.3),\n",
    "            A.RandomBrightnessContrast(p=0.1),\n",
    "            A.ColorJitter(p=0.1),\n",
    "            ToTensorV2()\n",
    "        ], bbox_params=A.BboxParams(format='coco'))\n",
    "    else:\n",
    "         transform = A.Compose([\n",
    "             A.Resize(image_size[0], image_size[1]), \n",
    "             ToTensorV2()\n",
    "         ], bbox_params=A.BboxParams(format='coco'))\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "class PotholeDetectionClass(datasets.VisionDataset):\n",
    "    def __init__(self, root, stage='/train', transform=None, target_transform=None, transforms=None, batch_size = batch_size):\n",
    "        super().__init__(root, transforms, transform, target_transform)\n",
    "        self.stage = stage #train, valid, test\n",
    "        self.coco = COCO(root + stage + \"/_annotations.coco.json\") # annotations stored here\n",
    "        self.ids = list(sorted(self.coco.imgs.keys()))\n",
    "        self.ids = [id for id in self.ids if (len(self._load_target(id)) > 0)]\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def _load_image(self, id: int):\n",
    "        path = self.coco.loadImgs(id)[0]['file_name']\n",
    "        path = \"/\" + path\n",
    "        image = cv2.imread(self.root + self.stage + path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        return image\n",
    "    def _load_target(self, id):\n",
    "        return self.coco.loadAnns(self.coco.getAnnIds(id))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        id = self.ids[index]\n",
    "        image = self._load_image(id)\n",
    "        target = self._load_target(id)\n",
    "        target = copy.deepcopy(self._load_target(id))\n",
    "\n",
    "        boxes = [t['bbox'] + [t['category_id']] for t in target]\n",
    "\n",
    "        transformed = self.transforms(image=image, bboxes=boxes)\n",
    "        image = transformed['image']\n",
    "        boxes = transformed['bboxes']\n",
    "\n",
    "        new_boxes = []\n",
    "        for box in boxes:\n",
    "            xmin = box[0]\n",
    "            xmax = xmin + box[2]\n",
    "            ymin = box[1]\n",
    "            ymax = ymin + box[3]\n",
    "            new_boxes.append([xmin, ymin, xmax, ymax])\n",
    "\n",
    "        boxes = torch.tensor(new_boxes, dtype=torch.float32)\n",
    "\n",
    "        targ = {}\n",
    "        targ['boxes'] = boxes\n",
    "        targ['labels'] = torch.tensor([t['category_id'] for t in target], dtype=torch.int64)\n",
    "        targ['image_id'] = torch.tensor([t['image_id'] for t in target])\n",
    "        targ['area'] = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0]) # we have a different area\n",
    "        targ['iscrowd'] = torch.tensor([t['iscrowd'] for t in target], dtype=torch.int64)\n",
    "        image = torch.tensor(image, dtype=torch.float32).div(255)\n",
    "\n",
    "           # Convert target dictionary into tensors\n",
    "        targ = {key: torch.tensor(val) for key, val in targ.items()}\n",
    "        return image, targ # scale images\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "    \n",
    "dataset_path = \"/Pothole_coco\"\n",
    "dataset_path = os.getcwd() + dataset_path\n",
    "\n",
    "coco = COCO(dataset_path + \"/train\" + \"/_annotations.coco.json\")\n",
    "categories = coco.cats\n",
    "n_classes = len(categories.keys())\n",
    "\n",
    "train_dataset = PotholeDetectionClass(root=dataset_path, transforms=get_transforms(True))\n",
    "test_dataset = PotholeDetectionClass(root=dataset_path, stage='/test', transforms=get_transforms(True))\n",
    "valid_dataset = PotholeDetectionClass(root=dataset_path, stage= \"/valid\", transform=get_transforms(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 667,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model = models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=True)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "model.roi_heads.box_predictor = models.detection.faster_rcnn.FastRCNNPredictor(in_features, n_classes)\n",
    "\n",
    "# Cuda\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 668,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn=custom_collate)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 669,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.01, momentum=0.9, nesterov=True, weight_decay=1e-4)\n",
    "#optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "#optimizer = torch.optim.PSO(params, lr=0.001) #particle swarm optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(img_tensor, annotation,phase='train'):\n",
    "\n",
    "    fig,ax = plt.subplots(1)\n",
    "    img = img_tensor.cpu()\n",
    "\n",
    "    ax.imshow(img.permute(1, 2, 0))\n",
    "    for idx,box in enumerate(annotation[\"boxes\"]):\n",
    "      \n",
    "      xmin, ymin, xmax, ymax = box\n",
    "      color=['r','g','b','r']\n",
    "      classes=['no mask','Masked','Improper masking','No-mask']\n",
    "      # Create a Rectangle patch\n",
    "      rect = patches.Rectangle((xmin,ymin),(xmax-xmin),(ymax-ymin),linewidth=3,edgecolor=color[annotation['labels'][idx]],facecolor='none')\n",
    "      ax.text(xmin, ymin, classes[annotation['labels'][idx]],color='black',bbox=dict(facecolor=color[annotation['labels'][idx]], alpha=0.1))\n",
    "      # Add the patch to the Axes\n",
    "      ax.add_patch(rect)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def accuracyMetric(preds,annotations):\n",
    "    non_accurate=0\n",
    "    accurate=0\n",
    "    def csm(A,B,corr):\n",
    "        if corr:\n",
    "            B=B-B.mean(axis=1)[:,np.newaxis]\n",
    "            A=A-A.mean(axis=1)[:,np.newaxis]\n",
    "        num=np.dot(A,B.T)\n",
    "        p1=np.sqrt(np.sum(A**2,axis=1))[:,np.newaxis]\n",
    "        p2=np.sqrt(np.sum(B**2,axis=1))[np.newaxis,:]\n",
    "        return 1-(num/(p1*p2))\n",
    "    inds=torch.where((preds['scores'])>0.91)\n",
    "    distMatrix=csm(np.array(preds['boxes'][inds].cpu()),np.array(annotations['boxes'].cpu()),True)\n",
    "\n",
    "    for i in range (distMatrix.shape[0]):\n",
    "        cla=np.argmin(distMatrix[i,:])\n",
    "\n",
    "        if preds['labels'][i]%3==annotations['labels'][cla]:\n",
    "            accurate+=1\n",
    "        else:\n",
    "            non_accurate+=1\n",
    "    allSamp=np.max(((accurate+non_accurate),len(annotations['labels'])))\n",
    "    return (accurate/allSamp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 671,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(pl.LightningModule):\n",
    "    def __init__(self, model, optimizer, train_loader, test_loader):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.losses_dict = []\n",
    "        self.loss = nn.BCEWithLogitsLoss()  # Binary Cross-Entropy Loss\n",
    "        \n",
    "\n",
    "    # def forward(self, images, target):\n",
    "    #     return self.model(images, target)\n",
    "\n",
    "    def forward(self, x, annotations=None, phase='train'):\n",
    "        if phase=='train':\n",
    "            out=self.model(x,annotations)\n",
    "        else:\n",
    "            self.model.eval()\n",
    "            out=model(x[0])\n",
    "        # no activation and no softmax at the end\n",
    "        return out\n",
    "\n",
    "   \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, annotations = batch\n",
    "        imgs = list(img for img in imgs)\n",
    "        annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "        \n",
    "\n",
    "        # Forward pass\n",
    "        loss_dict = self(imgs,annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "        if wandb_on:\n",
    "            wandb.log({\"train/loss\": losses})\n",
    "        occurrences = np.count_nonzero(annotations[0]['labels'].cpu() == 2)\n",
    "        occurrences2 = np.count_nonzero(annotations[0]['labels'].cpu() == 1)\n",
    "        occurrences = occurrences/(occurrences2+1 )\n",
    "\n",
    "        if occurrences>=1:\n",
    "\n",
    "            occurrences=np.clip(occurrences,1,4)\n",
    "            loss_dict['loss_classifier']=occurrences*4*loss_dict['loss_classifier']\n",
    "            print(f'Weighted {occurrences}')\n",
    "\n",
    "        elif losses<0.2:\n",
    "            for k,v in zip(loss_dict,loss_dict.values()):\n",
    "                loss_dict[k]=v*0\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
    "        tensorboard_logs = {'train_loss': loss , 'classifier_loss': loss_dict['loss_classifier'],\n",
    "                            'box_reg_loss':loss_dict['loss_box_reg']}\n",
    "        # use key 'log'\n",
    "\n",
    "        return {\"loss\": loss, 'log': tensorboard_logs}\n",
    "\n",
    "    # def test_step(self,batch,batch_idx):\n",
    "    #     imgs, annotations = batch\n",
    "    #     #imgs = list(img for img in imgs)\n",
    "    #     annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "    #     preds = self(list([imgs]),phase='test')\n",
    "    #     #print(preds['labels'])\n",
    "        \n",
    "    \n",
    "\n",
    "        \n",
    "    #     plot_image(imgs[0], preds[0])\n",
    "    #     plot_image(imgs[0], annotations[0],phase='test')\n",
    "    #     self.log('accuracy',(accuracyMetric(preds[0], annotations[0])))\n",
    "    #     print((accuracyMetric(preds[0], annotations[0])))\n",
    "\n",
    "    # def test_step(self, batch, batch_idx):\n",
    "    #     imgs, annotations = batch\n",
    "    #     imgs = list(img for img in imgs)\n",
    "    #     annotations = [{k: v for k, v in t.items()} for t in annotations]\n",
    "\n",
    "    #     # Forward pass\n",
    "    #     loss_dict = self(imgs[0], annotations)\n",
    "    #     losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "    #     # Log the test loss\n",
    "    #     self.log('test_loss', losses, on_step=True, on_epoch=True, prog_bar=True)\n",
    "\n",
    "    #     return {\"test_loss\": losses}\n",
    "    def test_step(self,batch,batch_idx):\n",
    "        imgs, annotations = batch\n",
    "        #imgs = list(img for img in imgs)\n",
    "        annotations = [{k: v for k, v in annotations.items()}]\n",
    "        preds = self(list([imgs]),phase='test')\n",
    "        #print(preds['labels'])\n",
    "        \n",
    "        plot_image(imgs, preds[0])\n",
    "        plot_image(imgs, annotations[0],phase='test')\n",
    "        self.log('accuracy',(accuracyMetric(preds[0], annotations[0])))\n",
    "        print((accuracyMetric(preds[0], annotations[0])))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return self.optimizer\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return self.train_loader\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return self.test_loader\n",
    "\n",
    "    def load_from_checkpoint(cls, checkpoint_path, model, optimizer, train_loader, test_loader):\n",
    "        model = model.load_from_checkpoint(checkpoint_path)\n",
    "        return cls(model, optimizer, train_loader, test_loader)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 672,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type              | Params\n",
      "--------------------------------------------\n",
      "0 | model | FasterRCNN        | 18.9 M\n",
      "1 | loss  | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------\n",
      "18.9 M    Trainable params\n",
      "58.9 K    Non-trainable params\n",
      "18.9 M    Total params\n",
      "75.721    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/30 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_5472\\2770485202.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image, dtype=torch.float32).div(255)\n",
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_5472\\2770485202.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targ = {key: torch.tensor(val) for key, val in targ.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [00:18<00:00,  1.65it/s, v_num=68, train_loss_step=0.997, train_loss_epoch=0.586]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=1` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 30/30 [00:19<00:00,  1.52it/s, v_num=68, train_loss_step=0.997, train_loss_epoch=0.586]\n"
     ]
    }
   ],
   "source": [
    "lightning_module = NN(model, optimizer, train_loader, test_loader)\n",
    "# Initialize a Lightning Trainer\n",
    "if wandb_on:\n",
    "    wandb_logger = WandbLogger(project='Bachelor', job_type='train')\n",
    "    wandb_logger.watch(model, log=\"all\")\n",
    "    trainer = pl.Trainer(max_epochs=num_epochs,logger=wandb_logger)  # You can adjust the Trainer options\n",
    "else:\n",
    "    trainer = pl.Trainer(max_epochs=num_epochs)\n",
    "# Start training\n",
    "\n",
    "\n",
    "trainer.fit(lightning_module, train_loader)\n",
    "\n",
    "if wandb_on:\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 673,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_5472\\2770485202.py:47: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  image = torch.tensor(image, dtype=torch.float32).div(255)\n",
      "C:\\Users\\aksel\\AppData\\Local\\Temp\\ipykernel_5472\\2770485202.py:50: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  targ = {key: torch.tensor(val) for key, val in targ.items()}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[673], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\trainer.py:754\u001b[0m, in \u001b[0;36mTrainer.test\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_test_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\call.py:44\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[1;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[0;32m     47\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\trainer.py:794\u001b[0m, in \u001b[0;36mTrainer._test_impl\u001b[1;34m(self, model, dataloaders, ckpt_path, verbose, datamodule)\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    791\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[0;32m    792\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn, ckpt_path, model_provided\u001b[38;5;241m=\u001b[39mmodel_provided, model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    793\u001b[0m )\n\u001b[1;32m--> 794\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[38;5;66;03m# remove the tensors from the test results\u001b[39;00m\n\u001b[0;32m    796\u001b[0m results \u001b[38;5;241m=\u001b[39m convert_tensors_to_scalars(results)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\trainer.py:989\u001b[0m, in \u001b[0;36mTrainer._run\u001b[1;34m(self, model, ckpt_path)\u001b[0m\n\u001b[0;32m    984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[0;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    987\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m--> 989\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m    994\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\trainer.py:1028\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mzero_grad(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mzero_grad_kwargs)\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluating:\n\u001b[1;32m-> 1028\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredicting:\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\loops\\utilities.py:182\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[1;32m--> 182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:134\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mis_last_batch \u001b[38;5;241m=\u001b[39m data_fetcher\u001b[38;5;241m.\u001b[39mdone\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;66;03m# run step hooks\u001b[39;00m\n\u001b[1;32m--> 134\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# this needs to wrap the `*_step` call too (not just `next`) for `dataloader_iter` support\u001b[39;00m\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\loops\\evaluation_loop.py:391\u001b[0m, in \u001b[0;36m_EvaluationLoop._evaluation_step\u001b[1;34m(self, batch, batch_idx, dataloader_idx, dataloader_iter)\u001b[0m\n\u001b[0;32m    385\u001b[0m hook_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mtesting \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalidation_step\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    386\u001b[0m step_args \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_step_args_from_hook_kwargs(hook_kwargs, hook_name)\n\u001b[0;32m    388\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_dataloader_iter\n\u001b[0;32m    389\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m (dataloader_iter,)\n\u001b[0;32m    390\u001b[0m )\n\u001b[1;32m--> 391\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mstep_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_progress\u001b[38;5;241m.\u001b[39mincrement_processed()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m using_dataloader_iter:\n\u001b[0;32m    396\u001b[0m     \u001b[38;5;66;03m# update the hook kwargs now that the step method might have consumed the iterator\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\trainer\\call.py:309\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[1;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 309\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[0;32m    312\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\pytorch_lightning\\strategies\\strategy.py:416\u001b[0m, in \u001b[0;36mStrategy.test_step\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module:\n\u001b[0;32m    415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m--> 416\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[671], line 89\u001b[0m, in \u001b[0;36mNN.test_step\u001b[1;34m(self, batch, batch_idx)\u001b[0m\n\u001b[0;32m     87\u001b[0m imgs, annotations \u001b[38;5;241m=\u001b[39m batch\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m#imgs = list(img for img in imgs)\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m annotations \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[43mannotations\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m()}]\n\u001b[0;32m     90\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;28mlist\u001b[39m([imgs]),phase\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#print(preds['labels'])\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "trainer.test(lightning_module, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
