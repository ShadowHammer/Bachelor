LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
  | Name     | Type               | Params
------------------------------------------------
0 | conv1    | Conv2d             | 896
1 | conv2    | Conv2d             | 9.2 K
2 | conv3    | Conv2d             | 18.5 K
3 | conv4    | Conv2d             | 36.9 K
4 | pool1    | MaxPool2d          | 0
5 | pool2    | MaxPool2d          | 0
6 | fc1      | Linear             | 819 K
7 | fc2      | Linear             | 65.7 K
8 | fc3      | Linear             | 1.3 K
9 | accuracy | MulticlassAccuracy | 0
------------------------------------------------
952 K     Trainable params
0         Non-trainable params
952 K     Total params
3.809     Total estimated model params size (MB)
C:\Users\tobia\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Sanity Checking DataLoader 0: 100%|##########| 2/2 [00:00<00:00,  4.06it/s]







Epoch 0: 100%|##########| 1407/1407 [00:14<00:00, 95.90it/s, v_num=kwzs]






Epoch 1: 100%|##########| 1407/1407 [00:13<00:00, 105.50it/s, v_num=kwzs, val_loss=1.470, val_acc=0.459]







Epoch 2: 100%|##########| 1407/1407 [00:14<00:00, 97.70it/s, v_num=kwzs, val_loss=1.300, val_acc=0.531]















Epoch 4: 100%|##########| 1407/1407 [00:15<00:00, 91.07it/s, v_num=kwzs, val_loss=1.090, val_acc=0.614]







Epoch 5: 100%|##########| 1407/1407 [00:15<00:00, 91.98it/s, v_num=kwzs, val_loss=1.010, val_acc=0.645]








Epoch 6: 100%|##########| 1407/1407 [00:16<00:00, 87.22it/s, v_num=kwzs, val_loss=0.942, val_acc=0.669]
















Epoch 8: 100%|##########| 1407/1407 [00:15<00:00, 91.37it/s, v_num=kwzs, val_loss=0.854, val_acc=0.700]







Epoch 9: 100%|##########| 1407/1407 [00:15<00:00, 89.67it/s, v_num=kwzs, val_loss=0.806, val_acc=0.728]
Files already downloaded and verified
`Trainer.fit` stopped: `max_epochs=10` reached.
C:\Users\tobia\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\pytorch_lightning\trainer\connectors\checkpoint_connector.py:145: `.test(ckpt_path=None)` was called without a model. The best model of the previous `fit` call will be used. You can pass `.test(ckpt_path='best')` to use the best model or `.test(ckpt_path='last')` to use the last model. If you pass a value, this warning will be silenced.
Restoring states from the checkpoint path at .\wandb-lightning\clklkwzs\checkpoints\epoch=9-step=14070.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Loaded model weights from the checkpoint at .\wandb-lightning\clklkwzs\checkpoints\epoch=9-step=14070.ckpt
C:\Users\tobia\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\pytorch_lightning\trainer\connectors\data_connector.py:441: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.
Files already downloaded and verified

Testing DataLoader 0: 100%|##########| 313/313 [00:02<00:00, 134.05it/s]
------------------------------------------------------------------------------------------------------------------------
       Test metric             DataLoader 0
------------------------------------------------------------------------------------------------------------------------
        test_acc            0.7200999855995178
        test_loss           0.8216265439987183
------------------------------------------------------------------------------------------------------------------------